from numpy.typing import NDArray
from numpy.lib.stride_tricks import sliding_window_view
from sklearn.preprocessing import RobustScaler, MinMaxScaler
import joblib
import pandas as pd
import numpy as np

selected_features = [
    # 1. æ ¸å¿ƒä»·æ ¼ç‰¹å¾ï¼ˆ2ä¸ªï¼‰
    "n_close",  # æ ‡å‡†åŒ–åçš„æ”¶ç›˜ä»·
    "n_midprice",  # æ ‡å‡†åŒ–åçš„ä¸­é—´ä»·
    # 2. å¸‚åœºå¾®è§‚ç»“æ„ï¼ˆ5ä¸ªï¼‰
    "bid_ask_spread",  # ä¹°å–ä»·å·®
    "size_imbalance_1",  # ä¸€æ¡£ä¹°å–é‡ä¸å¹³è¡¡
    "microprice",  # å¾®è§‚ä»·æ ¼ï¼ˆè€ƒè™‘æ·±åº¦çš„åŠ æƒä»·æ ¼ï¼‰
    "size_imbalance_5",
    # "order_flow_imbalance",  # è®¢å•æµä¸å¹³è¡¡
    "total_depth",  # æ€»å¸‚åœºæ·±åº¦
    # 3. åŠ¨é‡ä¸è¶‹åŠ¿ï¼ˆ4ä¸ªï¼‰
    "midprice_momentum_20",  # 20æœŸåŠ¨é‡
    "macd",  # MACDçº¿
    "ma_cross_5_20",  # ç§»åŠ¨å¹³å‡çº¿äº¤å‰ä¿¡å·
    "price_acceleration",  # ä»·æ ¼åŠ é€Ÿåº¦
    # 4. æ³¢åŠ¨ç‡ç‰¹å¾ï¼ˆ3ä¸ªï¼‰
    "volatility_20",  # 20æœŸæ³¢åŠ¨ç‡
    "bollinger_width",  # å¸ƒæ—å¸¦å®½åº¦
    "parkinson_vol_20",  # Parkinsonæ³¢åŠ¨ç‡ï¼ˆæ›´å‡†ç¡®çš„é«˜ä½ä»·ä¼°è®¡ï¼‰
    # 5. æŠ€æœ¯æŒ‡æ ‡ï¼ˆ3ä¸ªï¼‰
    "rsi_14",  # 14æœŸRSI
    "stochastic_k",  # éšæœºæŒ‡æ ‡Kå€¼
    "bias_20",  # 20æœŸä¹–ç¦»ç‡
    # 6. æˆäº¤é‡ä¸æµåŠ¨æ€§ï¼ˆ2ä¸ªï¼‰
    "amount_delta",  # æˆäº¤é¢å˜åŒ–
    "volume_momentum",  # æˆäº¤é‡åŠ¨é‡
    # 7. æ—¶é—´ç‰¹å¾ï¼ˆ1ä¸ªï¼‰
    "time_sin",  # æ—¶é—´æ­£å¼¦ç¼–ç 
    "sym",
]


def create_all_features(df: pd.DataFrame):
    """åˆ›å»ºæ‰€æœ‰ç‰¹å¾"""
    print("å¼€å§‹ç‰¹å¾å·¥ç¨‹...")

    df["n_midprice"] = df["n_midprice"] + 1
    df["n_close"] = df["n_close"] + 1
    df["n_bid1"] = df["n_bid1"] + 1
    df["n_bid2"] = df["n_bid2"] + 1
    df["n_bid3"] = df["n_bid3"] + 1
    df["n_bid4"] = df["n_bid4"] + 1
    df["n_bid5"] = df["n_bid5"] + 1
    df["n_ask1"] = df["n_ask1"] + 1
    df["n_ask2"] = df["n_ask2"] + 1
    df["n_ask3"] = df["n_ask3"] + 1
    df["n_ask4"] = df["n_ask4"] + 1
    df["n_ask5"] = df["n_ask5"] + 1
    time_series = pd.to_datetime(df["time"], format="%H:%M:%S")

    df["time_sin"] = np.sin(
        2
        * np.pi
        * (
            time_series.dt.hour * 3600
            + time_series.dt.minute * 60
            + time_series.dt.second
        )
        / 86400
    )

    df["bid_ask_spread"] = df["n_ask1"] - df["n_bid1"]

    df["bid_depth"] = sum([df[f"n_bsize{i}"] for i in range(1, 6)])
    df["ask_depth"] = sum([df[f"n_asize{i}"] for i in range(1, 6)])
    df["total_depth"] = df["bid_depth"] + df["ask_depth"]
    df["depth_imbalance"] = (df["bid_depth"] - df["ask_depth"]) / df["total_depth"]
    for depth in [1, 5]:
        df[f"size_imbalance_{depth}"] = (
            df[f"n_bsize{depth}"] - df[f"n_asize{depth}"]
        ) / (df[f"n_bsize{depth}"] + df[f"n_asize{depth}"])
    total_bid_size = sum([df[f"n_bsize{i}"] for i in range(1, 6)])
    total_ask_size = sum([df[f"n_asize{i}"] for i in range(1, 6)])
    df["microprice"] = (
        df["n_bid1"] * total_ask_size + df["n_ask1"] * total_bid_size
    ) / (total_bid_size + total_ask_size)

    df[f"midprice_momentum_20"] = df["n_midprice"] - df["n_midprice"].shift(20)

    exp1 = df["n_midprice"].ewm(span=12, adjust=False).mean()
    exp2 = df["n_midprice"].ewm(span=26, adjust=False).mean()
    df["macd"] = exp1 - exp2

    df["price_acceleration"] = (df["n_midprice"] - df["n_midprice"].shift(1)) - (
        df["n_midprice"] - df["n_midprice"].shift(1)
    ).shift(1)

    ma_periods = [5, 20]
    for period in ma_periods:
        df[f"ma_{period}"] = df["n_midprice"].rolling(window=period).mean()
        df[f"price_vs_ma_{period}"] = df["n_midprice"] / df[f"ma_{period}"] - 1

    # 4. ç§»åŠ¨å¹³å‡çº¿äº¤å‰
    df["ma_cross_5_20"] = df["ma_5"] - df["ma_20"]
    df["bias_20"] = (df["n_midprice"] - df["ma_20"]) / df["ma_20"] * 100

    rolling_window = 50
    df["midprice_ma"] = df["n_midprice"].rolling(window=rolling_window).mean()
    df["midprice_std"] = df["n_midprice"].rolling(window=rolling_window).std()
    df["bollinger_upper"] = df["midprice_ma"] + 2 * df["midprice_std"]
    df["bollinger_lower"] = df["midprice_ma"] - 2 * df["midprice_std"]
    df["bollinger_width"] = (df["bollinger_upper"] - df["bollinger_lower"]) / df[
        "midprice_ma"
    ]

    delta = df["n_midprice"].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    df["rsi_14"] = np.where((gain + loss) != 0, 1 - (loss / (gain + loss)), 0.5)

    low_min = df["n_midprice"].rolling(window=14).min()
    high_max = df["n_midprice"].rolling(window=14).max()
    price_diff = df["n_midprice"] - low_min
    range_diff = high_max - low_min
    df["stochastic_k"] = np.where(range_diff != 0, price_diff / range_diff, 0.5)

    df["volume_momentum"] = df["amount_delta"] - df["amount_delta"].shift(1)

    df["midprice_return"] = df["n_midprice"].pct_change()
    df["volatility_20"] = df["midprice_return"].rolling(window=20).std() * np.sqrt(20)
    high_low_ratio = (
        np.log(
            df["n_midprice"].rolling(window=2).max()
            / df["n_midprice"].rolling(window=2).min()
        )
        ** 2
    )
    df["parkinson_vol_20"] = np.sqrt(
        (1 / (4 * 20 * np.log(2))) * high_low_ratio.rolling(window=20).sum()
    )
    df["amount_delta"] = df["amount_delta"].apply(
        lambda x: np.sign(x) * np.log1p(np.abs(x))
    )
    df["volume_momentum"] = df["volume_momentum"].apply(
        lambda x: np.sign(x) * np.log1p(np.abs(x))
    )

    print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ")

    return df


def split(X, y, test_size=0.2):
    n_samples = X.shape[0]
    split_idx = int(n_samples * (1 - test_size))
    X_train = X[:split_idx, :, :]
    y_train = y[:split_idx]
    X_test = X[split_idx:, :, :]
    y_test = y[split_idx:]

    return X_train, X_test, y_train, y_test


def add_midprice_label(df: pd.DataFrame, time_delay: int):
    df[f"midprice_after_{time_delay}"] = df["n_midprice"].shift(-time_delay)
    return df


def sequentialize_certain_features(
    df: pd.DataFrame, feature_columns: list[str], label_column: str, seq_length: int
):
    """åˆ›å»ºåºåˆ—ç‰¹å¾ï¼ˆç”¨äºLSTMï¼‰"""
    print(f"Sequentializing features, sequence length: {seq_length}")

    features_np = df[feature_columns].values
    labels_np = df[label_column].values

    # åˆ›å»ºè§†å›¾
    X_view = sliding_window_view(features_np, (seq_length, features_np.shape[1]))
    X = X_view[:, 0, :, :]

    y = labels_np[seq_length - 1 :]

    # X, y = [], []
    # features = df[feature_columns]
    # for i in range(len(features) - seq_length):
    #     X.append(features[i : i + seq_length])
    #     y.append(df.iloc[i + seq_length - 1][label_column])
    print(f"Sequentializing features... done.")
    return X, y


def display_detail(df: pd.DataFrame, feature: str):
    print(df[feature].head())


def scale_train(scaler, X_train: NDArray):
    original_shape_train = X_train.shape

    # é‡å¡‘ä¸º2Dç”¨äºç¼©æ”¾ (samples*timesteps, features)
    n_features = X_train.shape[2]

    X_train_2d = X_train.reshape(-1, n_features)
    X_train_scaled_2d = scaler.fit_transform(X_train_2d)

    # é‡å¡‘å›3D (samples, timesteps, features)
    X_train_scaled = X_train_scaled_2d.reshape(original_shape_train)

    return X_train_scaled


def scale_test(scaler, X_test: NDArray):
    original_shape_test = X_test.shape
    n_features = X_test.shape[2]

    # é‡å¡‘ä¸º2Dç”¨äºç¼©æ”¾ (samples*timesteps, features)
    X_test_2d = X_test.reshape(-1, n_features)
    X_test_scaled_2d = scaler.transform(X_test_2d)

    # é‡å¡‘å›3D (samples, timesteps, features)
    X_test_scaled = X_test_scaled_2d.reshape(original_shape_test)

    return X_test_scaled


def scale(X_train: NDArray, X_test: NDArray):
    print("Scaling data...")

    balance_scaler = MinMaxScaler(feature_range=(-1, 1))

    volume_scaler = RobustScaler()
    X_train_scaled = np.concatenate(
        [
            scale_train(balance_scaler, X_train[:, :, 0:17]),
            scale_train(volume_scaler, X_train[:, :, 17:19]),
            X_train[:, :, 19:],
        ],
        axis=2,
    )
    # print(f"After Train Scaling: {price_scaler.center_,price_scaler.scale_}")
    X_test_scaled = np.concatenate(
        [
            scale_test(balance_scaler, X_test[:, :, 0:17]),
            scale_test(volume_scaler, X_test[:, :, 17:19]),
            X_test[:, :, 19:],
        ],
        axis=2,
    )
    # print(f"After Test Scaling: {price_scaler.center_,price_scaler.scale_}")

    joblib.dump(balance_scaler, "./balance.joblib")
    joblib.dump(volume_scaler, "./volume.joblib")
    print("The scalers saved to . ")

    print("Scaling data... Done.")
    return X_train_scaled, X_test_scaled


def inverse_scale(scaler, X_scaled: np.ndarray):
    original_shape = X_scaled.shape
    n_features = X_scaled.shape[-1]

    # å±•å¹³æˆ2Dè¿›è¡Œé€†å˜æ¢ï¼ˆä¸ç¼©æ”¾æ—¶ä¿æŒä¸€è‡´ï¼‰
    X_2d = X_scaled.reshape(-1, n_features)
    X_original_2d = scaler.inverse_transform(X_2d)

    # é‡å¡‘å›åŸå§‹å½¢çŠ¶
    X_original = X_original_2d.reshape(original_shape)

    return X_original


def comprehensive_scaler_selection(
    df, numerical_cols=None, skew_threshold=1.0, outlier_threshold=1.5
):
    """
    ç»¼åˆæ£€æŸ¥å¹¶æ¨èScalerçš„å®Œæ•´æµç¨‹
    """
    if numerical_cols is None:
        numerical_cols = df.select_dtypes(include=[np.number]).columns

    print("=" * 60)
    print("æ•°æ®é¢„å¤„ç†Scaleré€‰æ‹©åˆ†ææŠ¥å‘Š")
    print("=" * 60)

    # 1. æ£€æŸ¥ååº¦
    print("\nğŸ“Š 1. æ•°æ®åˆ†å¸ƒæ£€æŸ¥ï¼ˆååº¦åˆ†æï¼‰")
    print("-" * 40)
    skew_df = check_skewness(df, numerical_cols, skew_threshold)

    # 2. æ£€æŸ¥å¼‚å¸¸å€¼
    print("\nğŸ“Š 2. å¼‚å¸¸å€¼æ£€æŸ¥ï¼ˆIQRæ–¹æ³•ï¼‰")
    print("-" * 40)
    outliers_df = detect_outliers_iqr(df, numerical_cols, outlier_threshold)

    # 3. æ£€æŸ¥å€¼èŒƒå›´
    print("\nğŸ“Š 3. å€¼èŒƒå›´æ£€æŸ¥")
    print("-" * 40)
    range_df = check_value_ranges(df, numerical_cols)

    # 4. ç»¼åˆæ¨è
    print("\nğŸ¯ 4. ç»¼åˆScaleræ¨è")
    print("-" * 40)

    recommendations = {}
    for col in numerical_cols:
        # è·å–è¯¥ç‰¹å¾çš„å„é¡¹æ£€æŸ¥ç»“æœ
        skew_info = skew_df[skew_df["feature"] == col].iloc[0]
        outlier_info = outliers_df[outliers_df["feature"] == col].iloc[0]
        range_info = range_df[range_df["feature"] == col].iloc[0]

        # å†³ç­–é€»è¾‘
        if skew_info["is_highly_skewed"]:
            recommendations[col] = {
                "scaler": "PowerTransformer + StandardScaler",
                "reason": f"ä¸¥é‡åæ€ï¼ˆååº¦={skew_info['skewness']:.2f}ï¼‰",
            }
        elif outlier_info["is_high_outlier"]:
            recommendations[col] = {
                "scaler": "RobustScaler",
                "reason": f"å¼‚å¸¸å€¼è¾ƒå¤šï¼ˆ{outlier_info['outlier_percentage']:.1f}%ï¼‰",
            }
        elif range_info["has_clear_bounds"]:
            recommendations[col] = {
                "scaler": "MinMaxScaler",
                "reason": f"æœ‰æ˜ç¡®è¾¹ç•Œï¼ˆ{range_info['bound_type']}ï¼‰",
            }
        else:
            recommendations[col] = {
                "scaler": "StandardScaler",
                "reason": "åˆ†å¸ƒç›¸å¯¹æ­£å¸¸ï¼Œæ— æ˜æ˜¾å¼‚å¸¸å€¼",
            }

    # æ‰“å°æ¨èç»“æœ
    rec_df = pd.DataFrame.from_dict(recommendations, orient="index")
    rec_df.index.name = "feature"
    rec_df.reset_index(inplace=True)

    print("\næ¨èæ–¹æ¡ˆæ±‡æ€»:")
    print(rec_df.to_string(index=False))

    # ç»Ÿè®¡å„Scalerä½¿ç”¨é¢‘ç‡
    scaler_counts = rec_df["scaler"].value_counts()
    print(f"\nğŸ“ˆ Scalerä½¿ç”¨ç»Ÿè®¡:")
    for scaler, count in scaler_counts.items():
        print(f"  {scaler}: {count}ä¸ªç‰¹å¾")

    # ç»™å‡ºæœ€ç»ˆå»ºè®®
    if len(scaler_counts) == 1:
        print(f"\nâœ… å»ºè®®æ‰€æœ‰ç‰¹å¾ä½¿ç”¨: {scaler_counts.index[0]}")
    else:
        print(f"\nâš ï¸ å»ºè®®ä½¿ç”¨æ··åˆScalerï¼ˆä¸åŒç‰¹å¾ä½¿ç”¨ä¸åŒScalerï¼‰")
        print("å¯ä»¥ä½¿ç”¨ColumnTransformer:")
        print(
            """
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer

preprocessor = ColumnTransformer(
    transformers=[
        ('power', PowerTransformer(), [åæ€ç‰¹å¾åˆ—è¡¨]),
        ('robust', RobustScaler(), [å¼‚å¸¸å€¼å¤šçš„ç‰¹å¾åˆ—è¡¨]),
        ('minmax', MinMaxScaler(), [æœ‰è¾¹ç•Œç‰¹å¾åˆ—è¡¨]),
        ('standard', StandardScaler(), [å…¶ä»–ç‰¹å¾])
    ])
        """
        )

    return {
        "skew_df": skew_df,
        "outliers_df": outliers_df,
        "range_df": range_df,
        "recommendations": rec_df,
    }


def check_value_ranges(df, numerical_cols=None):
    """
    æ£€æŸ¥æ•°å€¼èŒƒå›´ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰æ˜ç¡®ç‰©ç†è¾¹ç•Œ

    å¸¸è§æœ‰æ˜ç¡®è¾¹ç•Œçš„ç‰¹å¾ï¼š
    - ç™¾åˆ†æ¯”ï¼š0-100
    - æ¦‚ç‡ï¼š0-1
    - å¹´é¾„ï¼š0-150
    - è¯„åˆ†ï¼š1-5, 1-10
    - äºŒå€¼ç‰¹å¾ï¼š0/1
    """
    if numerical_cols is None:
        numerical_cols = df.select_dtypes(include=[np.number]).columns

    range_results = []

    # å¸¸è§è¾¹ç•Œæ¡ä»¶
    common_bounds = {
        "percentage": (0, 100),
        "probability": (0, 1),
        "rating_5": (1, 5),
        "rating_10": (1, 10),
        "binary": (0, 1),
        "age": (0, 150),
    }

    for col in numerical_cols:
        data = df[col].dropna()
        min_val = data.min()
        max_val = data.max()
        range_val = max_val - min_val

        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆå¸¸è§è¾¹ç•Œ
        has_clear_bounds = False
        bound_type = None

        for bound_name, (lower, upper) in common_bounds.items():
            if min_val >= lower and max_val <= upper:
                has_clear_bounds = True
                bound_type = bound_name
                break

        # è‡ªå®šä¹‰è¾¹ç•Œæ£€æŸ¥ï¼ˆæ ¹æ®ä¸šåŠ¡çŸ¥è¯†ï¼‰
        # ä¾‹å¦‚ï¼šå¦‚æœæ•°æ®åœ¨[0, 255]ä¹‹é—´ï¼Œå¯èƒ½æ˜¯å›¾åƒåƒç´ 

        range_results.append(
            {
                "feature": col,
                "min": min_val,
                "max": max_val,
                "range": range_val,
                "has_clear_bounds": has_clear_bounds,
                "bound_type": bound_type,
                "recommendation": (
                    "MinMaxScaler" if has_clear_bounds else "æ ¹æ®åˆ†å¸ƒé€‰æ‹©"
                ),
            }
        )

    range_df = pd.DataFrame(range_results)

    # æ‰“å°æœ‰æ˜ç¡®è¾¹ç•Œçš„ç‰¹å¾
    bounded_features = range_df[range_df["has_clear_bounds"]]
    if len(bounded_features) > 0:
        print(f"âœ… å‘ç° {len(bounded_features)} ä¸ªæœ‰æ˜ç¡®è¾¹ç•Œçš„ç‰¹å¾:")
        print(
            bounded_features[["feature", "min", "max", "bound_type", "recommendation"]]
        )
        print("\nè¿™äº›ç‰¹å¾é€‚åˆä½¿ç”¨MinMaxScaler")

    return range_df


def detect_outliers_iqr(df, numerical_cols=None, threshold=1.5):
    """
    ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼

    thresholdé€šå¸¸å–1.5ï¼ˆä¸­åº¦å¼‚å¸¸ï¼‰æˆ–3ï¼ˆæç«¯å¼‚å¸¸ï¼‰
    """
    if numerical_cols is None:
        numerical_cols = df.select_dtypes(include=[np.number]).columns

    outliers_results = []

    for col in numerical_cols:
        data = df[col].dropna()

        # è®¡ç®—Q1, Q3, IQR
        Q1 = np.percentile(data, 25)
        Q3 = np.percentile(data, 75)
        IQR = Q3 - Q1

        # å¼‚å¸¸å€¼è¾¹ç•Œ
        lower_bound = Q1 - threshold * IQR
        upper_bound = Q3 + threshold * IQR

        # æ£€æµ‹å¼‚å¸¸å€¼
        outliers = data[(data < lower_bound) | (data > upper_bound)]
        n_outliers = len(outliers)
        outlier_percentage = n_outliers / len(data) * 100

        outliers_results.append(
            {
                "feature": col,
                "q1": Q1,
                "q3": Q3,
                "iqr": IQR,
                "lower_bound": lower_bound,
                "upper_bound": upper_bound,
                "n_outliers": n_outliers,
                "outlier_percentage": outlier_percentage,
                "is_high_outlier": outlier_percentage > 5,  # è¶…è¿‡5%è§†ä¸ºæœ‰å¤§é‡å¼‚å¸¸å€¼
                "recommendation": (
                    "RobustScaler" if outlier_percentage > 5 else "StandardScaler"
                ),
            }
        )

    outliers_df = pd.DataFrame(outliers_results)

    # æ‰“å°æœ‰å¤§é‡å¼‚å¸¸å€¼çš„ç‰¹å¾
    high_outlier_features = outliers_df[outliers_df["is_high_outlier"]]
    if len(high_outlier_features) > 0:
        print(f"âš ï¸ å‘ç° {len(high_outlier_features)} ä¸ªç‰¹å¾æœ‰å¤§é‡å¼‚å¸¸å€¼ï¼ˆ>5%ï¼‰:")
        print(
            high_outlier_features[["feature", "outlier_percentage", "recommendation"]]
        )
        print("\næ¨èä½¿ç”¨RobustScalerå¤„ç†è¿™äº›ç‰¹å¾")
    else:
        print(f"âœ… å¼‚å¸¸å€¼æ¯”ä¾‹æ­£å¸¸ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨StandardScaler")

    return outliers_df


# æ–¹æ³•3ï¼šZ-scoreæ–¹æ³•ï¼ˆé€‚åˆè¿‘ä¼¼æ­£æ€åˆ†å¸ƒï¼‰
def detect_outliers_zscore(df, numerical_cols=None, threshold=3):
    """ä½¿ç”¨Z-scoreæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼"""
    outliers_results = []

    for col in numerical_cols:
        data = df[col].dropna()
        z_scores = np.abs(stats.zscore(data))
        outliers = data[z_scores > threshold]
        outlier_percentage = len(outliers) / len(data) * 100

        outliers_results.append(
            {
                "feature": col,
                "outlier_percentage": outlier_percentage,
                "is_high_outlier": outlier_percentage > 5,
            }
        )

    return pd.DataFrame(outliers_results)


def check_skewness(df, numerical_cols=None, threshold=1.0):
    """
    æ£€æŸ¥ååº¦ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦PowerTransformer

    ååº¦åˆ¤æ–­æ ‡å‡†ï¼š
    |Skewness| < 0.5: è¿‘ä¼¼å¯¹ç§°
    0.5 â‰¤ |Skewness| < 1: ä¸­ç­‰åæ€
    |Skewness| â‰¥ 1: ä¸¥é‡åæ€ï¼ˆéœ€è¦å¤„ç†ï¼‰
    """
    if numerical_cols is None:
        numerical_cols = df.select_dtypes(include=[np.number]).columns

    skewness_results = []
    for col in numerical_cols:
        skew = df[col].skew()
        is_highly_skewed = abs(skew) >= threshold
        skewness_results.append(
            {
                "feature": col,
                "skewness": skew,
                "abs_skewness": abs(skew),
                "is_highly_skewed": is_highly_skewed,
                "recommendation": (
                    "PowerTransformer" if is_highly_skewed else "StandardScaler"
                ),
            }
        )

    skew_df = pd.DataFrame(skewness_results)

    # æ‰“å°ä¸¥é‡åæ€çš„ç‰¹å¾
    highly_skewed = skew_df[skew_df["is_highly_skewed"]]
    if len(highly_skewed) > 0:
        print(f"âš ï¸ å‘ç° {len(highly_skewed)} ä¸ªä¸¥é‡åæ€ç‰¹å¾ï¼ˆ|ååº¦|â‰¥{threshold}ï¼‰:")
        print(highly_skewed[["feature", "skewness", "recommendation"]])
        print("\næ¨èå…ˆå¯¹è¿™äº›ç‰¹å¾ä½¿ç”¨PowerTransformerï¼Œç„¶åå†ç”¨StandardScaler")
    else:
        print(f"âœ… æ²¡æœ‰ä¸¥é‡åæ€ç‰¹å¾ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨StandardScaler")

    return skew_df


# ä½¿ç”¨ç¤ºä¾‹
# skew_df = check_skewness(df, threshold=1.0)
if __name__ == "__main__":
    df = pd.read_csv("./merged_data/merged_0.csv")
    df = create_all_features(df)
    results = comprehensive_scaler_selection(
        df, skew_threshold=1.0, outlier_threshold=1.5
    )
